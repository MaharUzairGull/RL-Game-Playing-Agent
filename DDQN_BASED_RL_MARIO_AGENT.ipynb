{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-super-mario-bros\n",
        "!pip install TensorDict\n",
        "!pip install torchrl\n",
        "!pip install gym[other]"
      ],
      "metadata": {
        "id": "R2w4Xxd-pACm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gym import Wrapper\n",
        "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
        "\n",
        "\n",
        "class SkipFrame(Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self.skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(self.skip):\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return next_state, total_reward, done, info\n",
        "\n",
        "\n",
        "def apply_wrappers(env):\n",
        "    env = SkipFrame(env, skip=4) # Num of frames to apply one action to\n",
        "    env = ResizeObservation(env, shape=84) # Resize frame from 240x256 to 84x84\n",
        "    env = GrayScaleObservation(env)\n",
        "    env = FrameStack(env, num_stack=4, lz4_compress=True) # May need to change lz4_compress to False if issues arise\n",
        "    return env"
      ],
      "metadata": {
        "id": "ey8G8IgswZnD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "class AgentNN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions, freeze=False):\n",
        "        super().__init__()\n",
        "        # Conolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        # Linear layers\n",
        "        self.network = nn.Sequential(\n",
        "            self.conv_layers,\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "        if freeze:\n",
        "            self._freeze()\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv_layers(torch.zeros(1, *shape))\n",
        "        # np.prod returns the product of array elements over a given axis\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def _freeze(self):\n",
        "        for p in self.network.parameters():\n",
        "            p.requires_grad = False\n",
        ""
      ],
      "metadata": {
        "id": "C_L7NopRr6b2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 input_dims,\n",
        "                 num_actions,\n",
        "                 lr=0.00025,\n",
        "                 gamma=0.9,\n",
        "                 epsilon=1.0,\n",
        "                 eps_decay=0.99999975,\n",
        "                 eps_min=0.1,\n",
        "                 replay_buffer_capacity=100_000,\n",
        "                 batch_size=32,\n",
        "                 sync_network_rate=10000):\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_decay = eps_decay\n",
        "        self.eps_min = eps_min\n",
        "        self.batch_size = batch_size\n",
        "        self.sync_network_rate = sync_network_rate\n",
        "\n",
        "        # Networks\n",
        "        self.online_network = AgentNN(input_dims, num_actions)\n",
        "        self.target_network = AgentNN(input_dims, num_actions, freeze=True)\n",
        "\n",
        "        # Optimizer and loss\n",
        "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=self.lr)\n",
        "        self.loss = torch.nn.MSELoss()\n",
        "        # self.loss = torch.nn.SmoothL1Loss() # Feel free to try this loss function instead!\n",
        "\n",
        "        # Replay buffer\n",
        "        storage = LazyMemmapStorage(replay_buffer_capacity)\n",
        "        self.replay_buffer = TensorDictReplayBuffer(storage=storage)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        # Passing in a list of numpy arrays is slower than creating a tensor from a numpy array\n",
        "        # Hence the `np.array(observation)` instead of `observation`\n",
        "        # observation is a LIST of numpy arrays because of the LazyFrame wrapper\n",
        "        # Unqueeze adds a dimension to the tensor, which represents the batch dimension\n",
        "        observation = torch.tensor(np.array(observation), dtype=torch.float32) \\\n",
        "                        .unsqueeze(0) \\\n",
        "                        .to(self.online_network.device)\n",
        "        # Grabbing the index of the action that's associated with the highest Q-value\n",
        "        return self.online_network(observation).argmax().item()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min)\n",
        "\n",
        "    def store_in_memory(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.add(TensorDict({\n",
        "                                            \"state\": torch.tensor(np.array(state), dtype=torch.float32),\n",
        "                                            \"action\": torch.tensor(action),\n",
        "                                            \"reward\": torch.tensor(reward),\n",
        "                                            \"next_state\": torch.tensor(np.array(next_state), dtype=torch.float32),\n",
        "                                            \"done\": torch.tensor(done)\n",
        "                                          }, batch_size=[]))\n",
        "\n",
        "    def sync_networks(self):\n",
        "        if self.learn_step_counter % self.sync_network_rate == 0 and self.learn_step_counter > 0:\n",
        "            self.target_network.load_state_dict(self.online_network.state_dict())\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.online_network.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.online_network.load_state_dict(torch.load(path))\n",
        "        self.target_network.load_state_dict(torch.load(path))\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.sync_networks()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        samples = self.replay_buffer.sample(self.batch_size).to(self.online_network.device)\n",
        "\n",
        "        keys = (\"state\", \"action\", \"reward\", \"next_state\", \"done\")\n",
        "\n",
        "        states, actions, rewards, next_states, dones = [samples[key] for key in keys]\n",
        "\n",
        "        predicted_q_values = self.online_network(states) # Shape is (batch_size, n_actions)\n",
        "        predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
        "\n",
        "        # Max returns two tensors, the first one is the maximum value, the second one is the index of the maximum value\n",
        "        target_q_values = self.target_network(next_states).max(dim=1)[0]\n",
        "        # The rewards of any future states don't matter if the current state is a terminal state\n",
        "        # If done is true, then 1 - done is 0, so the part after the plus sign (representing the future rewards) is 0\n",
        "        target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
        "\n",
        "        loss = self.loss(predicted_q_values, target_q_values)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.learn_step_counter += 1\n",
        "        self.decay_epsilon()\n",
        "\n"
      ],
      "metadata": {
        "id": "hj6d1qgQrjdT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def get_current_date_time_string():\n",
        "    return datetime.datetime.now().strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
        "\n",
        "\n",
        "class Timer():\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "\n",
        "    def start(self):\n",
        "        self.t = time.time()\n",
        "\n",
        "    def print(self, msg=''):\n",
        "        print(f\"Time taken: {msg}\", time.time() - self.t)\n",
        "\n",
        "    def get(self):\n",
        "        return time.time() - self.t\n",
        "\n",
        "    def store(self):\n",
        "        self.times.append(time.time() - self.t)\n",
        "\n",
        "    def average(self):\n",
        "        return sum(self.times) / len(self.times)"
      ],
      "metadata": {
        "id": "zNDUFFJRuO62"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import os\n",
        "\n",
        "\n",
        "model_path = os.path.join(\"models\", get_current_date_time_string())\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"CUDA is not available\")\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
        "SHOULD_TRAIN = True\n",
        "DISPLAY = True\n",
        "CKPT_SAVE_INTERVAL = 100\n",
        "NUM_OF_EPISODES = 1000\n",
        "\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = JoypadSpace(env, RIGHT_ONLY)\n",
        "\n",
        "env = apply_wrappers(env)\n",
        "\n",
        "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
        "\n",
        "if not SHOULD_TRAIN:\n",
        "    folder_name = \"\"\n",
        "    ckpt_name = \"\"\n",
        "    agent.load_model(os.path.join(\"models\", folder_name, ckpt_name))\n",
        "    agent.epsilon = 0.2\n",
        "    agent.eps_min = 0.0\n",
        "    agent.eps_decay = 0.0\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "\n",
        "for i in range(NUM_OF_EPISODES):\n",
        "    print(\"Episode:\", i)\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        a = agent.choose_action(state)\n",
        "        new_state, reward, done,  info  = env.step(a)\n",
        "        total_reward += reward\n",
        "\n",
        "        if SHOULD_TRAIN:\n",
        "            agent.store_in_memory(state, a, reward, new_state, done)\n",
        "            agent.learn()\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "    print(\"Total reward:\", total_reward, \"Epsilon:\", agent.epsilon, \"Size of replay buffer:\", len(agent.replay_buffer), \"Learn step counter:\", agent.learn_step_counter)\n",
        "\n",
        "    if SHOULD_TRAIN and (i + 1) % CKPT_SAVE_INTERVAL == 0:\n",
        "        agent.save_model(os.path.join(model_path, \"model_\" + str(i + 1) + \"_iter.pt\"))\n",
        "\n",
        "    print(\"Total reward:\", total_reward)\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNZoxvawo5FV",
        "outputId": "22ad91b1-7522-440c-fb9a-463373df4272"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA device: Tesla T4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0\n",
            "Total reward: 780.0 Epsilon: 0.9996458126948069 Size of replay buffer: 1448 Learn step counter: 1417\n",
            "Total reward: 780.0\n",
            "Episode: 1\n",
            "Total reward: 635.0 Epsilon: 0.99960532785415 Size of replay buffer: 1610 Learn step counter: 1579\n",
            "Total reward: 635.0\n",
            "Episode: 2\n",
            "Total reward: 673.0 Epsilon: 0.9994736385109522 Size of replay buffer: 2137 Learn step counter: 2106\n",
            "Total reward: 673.0\n",
            "Episode: 3\n",
            "Total reward: 231.0 Epsilon: 0.9994636438232898 Size of replay buffer: 2177 Learn step counter: 2146\n",
            "Total reward: 231.0\n",
            "Episode: 4\n",
            "Total reward: 1011.0 Epsilon: 0.9993617037178968 Size of replay buffer: 2585 Learn step counter: 2554\n",
            "Total reward: 1011.0\n",
            "Episode: 5\n",
            "Total reward: 236.0 Epsilon: 0.9993529593401517 Size of replay buffer: 2620 Learn step counter: 2589\n",
            "Total reward: 236.0\n",
            "Episode: 6\n",
            "Total reward: 1624.0 Epsilon: 0.9992223024662467 Size of replay buffer: 3143 Learn step counter: 3112\n",
            "Total reward: 1624.0\n",
            "Episode: 7\n",
            "Total reward: 628.0 Epsilon: 0.9991953238249177 Size of replay buffer: 3251 Learn step counter: 3220\n",
            "Total reward: 628.0\n",
            "Episode: 8\n",
            "Total reward: 882.0 Epsilon: 0.9989433085597076 Size of replay buffer: 4260 Learn step counter: 4229\n",
            "Total reward: 882.0\n",
            "Episode: 9\n",
            "Total reward: 224.0 Epsilon: 0.9989333191753189 Size of replay buffer: 4300 Learn step counter: 4269\n",
            "Total reward: 224.0\n",
            "Episode: 10\n",
            "Total reward: 601.0 Epsilon: 0.9986306881926473 Size of replay buffer: 5512 Learn step counter: 5481\n",
            "Total reward: 601.0\n",
            "Episode: 11\n",
            "Total reward: 1220.0 Epsilon: 0.9984666765514117 Size of replay buffer: 6169 Learn step counter: 6138\n",
            "Total reward: 1220.0\n",
            "Episode: 12\n",
            "Total reward: 613.0 Epsilon: 0.9984409663622932 Size of replay buffer: 6272 Learn step counter: 6241\n",
            "Total reward: 613.0\n",
            "Episode: 13\n",
            "Total reward: 773.0 Epsilon: 0.9983935415368048 Size of replay buffer: 6462 Learn step counter: 6431\n",
            "Total reward: 773.0\n",
            "Episode: 14\n",
            "Total reward: 693.0 Epsilon: 0.9983366347196755 Size of replay buffer: 6690 Learn step counter: 6659\n",
            "Total reward: 693.0\n",
            "Episode: 15\n",
            "Total reward: 595.0 Epsilon: 0.9982989482183339 Size of replay buffer: 6841 Learn step counter: 6810\n",
            "Total reward: 595.0\n",
            "Episode: 16\n",
            "Total reward: 984.0 Epsilon: 0.9978049124610786 Size of replay buffer: 8821 Learn step counter: 8790\n",
            "Total reward: 984.0\n",
            "Episode: 17\n",
            "Total reward: 650.0 Epsilon: 0.9977837093293195 Size of replay buffer: 8906 Learn step counter: 8875\n",
            "Total reward: 650.0\n",
            "Episode: 18\n",
            "Total reward: 598.0 Epsilon: 0.9977472908840125 Size of replay buffer: 9052 Learn step counter: 9021\n",
            "Total reward: 598.0\n",
            "Episode: 19\n",
            "Total reward: 621.0 Epsilon: 0.9976894232120778 Size of replay buffer: 9284 Learn step counter: 9253\n",
            "Total reward: 621.0\n",
            "Episode: 20\n",
            "Total reward: 1231.0 Epsilon: 0.9975046183413812 Size of replay buffer: 10025 Learn step counter: 9994\n",
            "Total reward: 1231.0\n",
            "Episode: 21\n",
            "Total reward: 239.0 Epsilon: 0.9974956408390914 Size of replay buffer: 10061 Learn step counter: 10030\n",
            "Total reward: 239.0\n",
            "Episode: 22\n",
            "Total reward: 1158.0 Epsilon: 0.997213389477865 Size of replay buffer: 11193 Learn step counter: 11162\n",
            "Total reward: 1158.0\n",
            "Episode: 23\n",
            "Total reward: 219.0 Epsilon: 0.9972031680917284 Size of replay buffer: 11234 Learn step counter: 11203\n",
            "Total reward: 219.0\n",
            "Episode: 24\n",
            "Total reward: 234.0 Epsilon: 0.9971931961086596 Size of replay buffer: 11274 Learn step counter: 11243\n",
            "Total reward: 234.0\n",
            "Episode: 25\n",
            "Total reward: 234.0 Epsilon: 0.9971847200014555 Size of replay buffer: 11308 Learn step counter: 11277\n",
            "Total reward: 234.0\n",
            "Episode: 26\n",
            "Total reward: 622.0 Epsilon: 0.9971281313672319 Size of replay buffer: 11535 Learn step counter: 11504\n",
            "Total reward: 622.0\n",
            "Episode: 27\n",
            "Total reward: 600.0 Epsilon: 0.9970897426683578 Size of replay buffer: 11689 Learn step counter: 11658\n",
            "Total reward: 600.0\n",
            "Episode: 28\n",
            "Total reward: 642.0 Epsilon: 0.9969369503541828 Size of replay buffer: 12302 Learn step counter: 12271\n",
            "Total reward: 642.0\n",
            "Episode: 29\n",
            "Total reward: 650.0 Epsilon: 0.9969147687510331 Size of replay buffer: 12391 Learn step counter: 12360\n",
            "Total reward: 650.0\n",
            "Episode: 30\n",
            "Total reward: 586.0 Epsilon: 0.996882120322892 Size of replay buffer: 12522 Learn step counter: 12491\n",
            "Total reward: 586.0\n",
            "Episode: 31\n",
            "Total reward: 226.0 Epsilon: 0.9968714038963584 Size of replay buffer: 12565 Learn step counter: 12534\n",
            "Total reward: 226.0\n",
            "Episode: 32\n",
            "Total reward: 640.0 Epsilon: 0.9968372616311919 Size of replay buffer: 12702 Learn step counter: 12671\n",
            "Total reward: 640.0\n",
            "Episode: 33\n",
            "Total reward: 795.0 Epsilon: 0.9967764564052015 Size of replay buffer: 12946 Learn step counter: 12915\n",
            "Total reward: 795.0\n",
            "Episode: 34\n",
            "Total reward: 627.0 Epsilon: 0.9967537799959254 Size of replay buffer: 13037 Learn step counter: 13006\n",
            "Total reward: 627.0\n",
            "Episode: 35\n",
            "Total reward: 250.0 Epsilon: 0.9967455568101321 Size of replay buffer: 13070 Learn step counter: 13039\n",
            "Total reward: 250.0\n",
            "Episode: 36\n",
            "Total reward: 592.0 Epsilon: 0.996720887659796 Size of replay buffer: 13169 Learn step counter: 13138\n",
            "Total reward: 592.0\n",
            "Episode: 37\n",
            "Total reward: 1513.0 Epsilon: 0.9964376099747778 Size of replay buffer: 14306 Learn step counter: 14275\n",
            "Total reward: 1513.0\n",
            "Episode: 38\n",
            "Total reward: 612.0 Epsilon: 0.9964161867937824 Size of replay buffer: 14392 Learn step counter: 14361\n",
            "Total reward: 612.0\n",
            "Episode: 39\n",
            "Total reward: 687.0 Epsilon: 0.9963489309626674 Size of replay buffer: 14662 Learn step counter: 14631\n",
            "Total reward: 687.0\n",
            "Episode: 40\n",
            "Total reward: 1375.0 Epsilon: 0.9962353536443546 Size of replay buffer: 15118 Learn step counter: 15087\n",
            "Total reward: 1375.0\n",
            "Episode: 41\n",
            "Total reward: 623.0 Epsilon: 0.996179316974746 Size of replay buffer: 15343 Learn step counter: 15312\n",
            "Total reward: 623.0\n",
            "Episode: 42\n",
            "Total reward: 243.0 Epsilon: 0.9961713475710906 Size of replay buffer: 15375 Learn step counter: 15344\n",
            "Total reward: 243.0\n",
            "Episode: 43\n",
            "Total reward: 248.0 Epsilon: 0.9961631291903456 Size of replay buffer: 15408 Learn step counter: 15377\n",
            "Total reward: 248.0\n",
            "Episode: 44\n",
            "Total reward: 1323.0 Epsilon: 0.9959967838171793 Size of replay buffer: 16076 Learn step counter: 16045\n",
            "Total reward: 1323.0\n",
            "Episode: 45\n",
            "Total reward: 222.0 Epsilon: 0.9959868238978944 Size of replay buffer: 16116 Learn step counter: 16085\n",
            "Total reward: 222.0\n",
            "Episode: 46\n",
            "Total reward: 603.0 Epsilon: 0.9959462382566798 Size of replay buffer: 16279 Learn step counter: 16248\n",
            "Total reward: 603.0\n",
            "Episode: 47\n",
            "Total reward: 753.0 Epsilon: 0.9958337026760543 Size of replay buffer: 16731 Learn step counter: 16700\n",
            "Total reward: 753.0\n",
            "Episode: 48\n",
            "Total reward: 243.0 Epsilon: 0.9958237443875729 Size of replay buffer: 16771 Learn step counter: 16740\n",
            "Total reward: 243.0\n",
            "Episode: 49\n",
            "Total reward: 1300.0 Epsilon: 0.9957570264234266 Size of replay buffer: 17039 Learn step counter: 17008\n",
            "Total reward: 1300.0\n",
            "Episode: 50\n",
            "Total reward: 660.0 Epsilon: 0.9955290241431016 Size of replay buffer: 17955 Learn step counter: 17924\n",
            "Total reward: 660.0\n",
            "Episode: 51\n",
            "Total reward: 248.0 Epsilon: 0.9955215577024854 Size of replay buffer: 17985 Learn step counter: 17954\n",
            "Total reward: 248.0\n",
            "Episode: 52\n",
            "Total reward: 1050.0 Epsilon: 0.995120194514406 Size of replay buffer: 19598 Learn step counter: 19567\n",
            "Total reward: 1050.0\n",
            "Episode: 53\n",
            "Total reward: 232.0 Epsilon: 0.9951112384718371 Size of replay buffer: 19634 Learn step counter: 19603\n",
            "Total reward: 232.0\n",
            "Episode: 54\n",
            "Total reward: 1241.0 Epsilon: 0.9949667090430665 Size of replay buffer: 20215 Learn step counter: 20184\n",
            "Total reward: 1241.0\n",
            "Episode: 55\n",
            "Total reward: 767.0 Epsilon: 0.994871445529568 Size of replay buffer: 20598 Learn step counter: 20567\n",
            "Total reward: 767.0\n",
            "Episode: 56\n",
            "Total reward: 640.0 Epsilon: 0.9948366256339686 Size of replay buffer: 20738 Learn step counter: 20707\n",
            "Total reward: 640.0\n",
            "Episode: 57\n",
            "Total reward: 926.0 Epsilon: 0.9946568251413445 Size of replay buffer: 21461 Learn step counter: 21430\n",
            "Total reward: 926.0\n",
            "Episode: 58\n",
            "Total reward: 623.0 Epsilon: 0.994602120513507 Size of replay buffer: 21681 Learn step counter: 21650\n",
            "Total reward: 623.0\n",
            "Episode: 59\n",
            "Total reward: 1011.0 Epsilon: 0.9945265336152274 Size of replay buffer: 21985 Learn step counter: 21954\n",
            "Total reward: 1011.0\n",
            "Episode: 60\n",
            "Total reward: 613.0 Epsilon: 0.9944892395648173 Size of replay buffer: 22135 Learn step counter: 22104\n",
            "Total reward: 613.0\n",
            "Episode: 61\n",
            "Total reward: 613.0 Epsilon: 0.994472830625685 Size of replay buffer: 22201 Learn step counter: 22170\n",
            "Total reward: 613.0\n",
            "Episode: 62\n",
            "Total reward: 630.0 Epsilon: 0.9944484663367498 Size of replay buffer: 22299 Learn step counter: 22268\n",
            "Total reward: 630.0\n",
            "Episode: 63\n",
            "Total reward: 626.0 Epsilon: 0.9943965077553111 Size of replay buffer: 22508 Learn step counter: 22477\n",
            "Total reward: 626.0\n",
            "Episode: 64\n",
            "Total reward: 785.0 Epsilon: 0.9943234222887689 Size of replay buffer: 22802 Learn step counter: 22771\n",
            "Total reward: 785.0\n",
            "Episode: 65\n",
            "Total reward: 906.0 Epsilon: 0.9941181156750106 Size of replay buffer: 23628 Learn step counter: 23597\n",
            "Total reward: 906.0\n",
            "Episode: 66\n",
            "Total reward: 544.0 Epsilon: 0.9938052661962213 Size of replay buffer: 24887 Learn step counter: 24856\n",
            "Total reward: 544.0\n",
            "Episode: 67\n",
            "Total reward: 234.0 Epsilon: 0.9937950797431737 Size of replay buffer: 24928 Learn step counter: 24897\n",
            "Total reward: 234.0\n",
            "Episode: 68\n",
            "Total reward: 640.0 Epsilon: 0.9937610428403184 Size of replay buffer: 25065 Learn step counter: 25034\n",
            "Total reward: 640.0\n",
            "Episode: 69\n",
            "Total reward: 947.0 Epsilon: 0.9936080154039862 Size of replay buffer: 25681 Learn step counter: 25650\n",
            "Total reward: 947.0\n",
            "Episode: 70\n",
            "Total reward: 638.0 Epsilon: 0.9935715009758079 Size of replay buffer: 25828 Learn step counter: 25797\n",
            "Total reward: 638.0\n",
            "Episode: 71\n",
            "Total reward: 708.0 Epsilon: 0.9935002147691157 Size of replay buffer: 26115 Learn step counter: 26084\n",
            "Total reward: 708.0\n",
            "Episode: 72\n",
            "Total reward: 243.0 Epsilon: 0.9934915216791809 Size of replay buffer: 26150 Learn step counter: 26119\n",
            "Total reward: 243.0\n",
            "Episode: 73\n",
            "Total reward: 673.0 Epsilon: 0.9933055077845635 Size of replay buffer: 26899 Learn step counter: 26868\n",
            "Total reward: 673.0\n",
            "Episode: 74\n",
            "Total reward: 228.0 Epsilon: 0.9932960714258815 Size of replay buffer: 26937 Learn step counter: 26906\n",
            "Total reward: 228.0\n",
            "Episode: 75\n",
            "Total reward: 217.0 Epsilon: 0.9932846485853122 Size of replay buffer: 26983 Learn step counter: 26952\n",
            "Total reward: 217.0\n",
            "Episode: 76\n",
            "Total reward: 602.0 Epsilon: 0.9932568370010355 Size of replay buffer: 27095 Learn step counter: 27064\n",
            "Total reward: 602.0\n",
            "Episode: 77\n",
            "Total reward: 624.0 Epsilon: 0.9932024576710522 Size of replay buffer: 27314 Learn step counter: 27283\n",
            "Total reward: 624.0\n",
            "Episode: 78\n",
            "Total reward: 615.0 Epsilon: 0.9931820972268175 Size of replay buffer: 27396 Learn step counter: 27365\n",
            "Total reward: 615.0\n",
            "Episode: 79\n",
            "Total reward: 635.0 Epsilon: 0.9931423707324921 Size of replay buffer: 27556 Learn step counter: 27525\n",
            "Total reward: 635.0\n",
            "Episode: 80\n",
            "Total reward: 627.0 Epsilon: 0.9931202735578101 Size of replay buffer: 27645 Learn step counter: 27614\n",
            "Total reward: 627.0\n",
            "Episode: 81\n",
            "Total reward: 556.0 Epsilon: 0.9928735138001227 Size of replay buffer: 28639 Learn step counter: 28608\n",
            "Total reward: 556.0\n",
            "Episode: 82\n",
            "Total reward: 619.0 Epsilon: 0.9928357853187225 Size of replay buffer: 28791 Learn step counter: 28760\n",
            "Total reward: 619.0\n",
            "Episode: 83\n",
            "Total reward: 597.0 Epsilon: 0.9928099719206495 Size of replay buffer: 28895 Learn step counter: 28864\n",
            "Total reward: 597.0\n",
            "Episode: 84\n",
            "Total reward: 1253.0 Epsilon: 0.9926642877288785 Size of replay buffer: 29482 Learn step counter: 29451\n",
            "Total reward: 1253.0\n",
            "Episode: 85\n",
            "Total reward: 636.0 Epsilon: 0.9926248301027291 Size of replay buffer: 29641 Learn step counter: 29610\n",
            "Total reward: 636.0\n",
            "Episode: 86\n",
            "Total reward: 805.0 Epsilon: 0.9925771852484075 Size of replay buffer: 29833 Learn step counter: 29802\n",
            "Total reward: 805.0\n",
            "Episode: 87\n",
            "Total reward: 783.0 Epsilon: 0.9922196736733218 Size of replay buffer: 31274 Learn step counter: 31243\n",
            "Total reward: 783.0\n",
            "Episode: 88\n",
            "Total reward: 637.0 Epsilon: 0.9921819700373776 Size of replay buffer: 31426 Learn step counter: 31395\n",
            "Total reward: 637.0\n",
            "Episode: 89\n",
            "Total reward: 237.0 Epsilon: 0.9921722963091187 Size of replay buffer: 31465 Learn step counter: 31434\n",
            "Total reward: 237.0\n",
            "Episode: 90\n",
            "Total reward: 603.0 Epsilon: 0.9921484844567708 Size of replay buffer: 31561 Learn step counter: 31530\n",
            "Total reward: 603.0\n",
            "Episode: 91\n",
            "Total reward: 601.0 Epsilon: 0.992105574957373 Size of replay buffer: 31734 Learn step counter: 31703\n",
            "Total reward: 601.0\n",
            "Episode: 92\n",
            "Total reward: 248.0 Epsilon: 0.9920973901191172 Size of replay buffer: 31767 Learn step counter: 31736\n",
            "Total reward: 248.0\n",
            "Episode: 93\n",
            "Total reward: 738.0 Epsilon: 0.9920569629691186 Size of replay buffer: 31930 Learn step counter: 31899\n",
            "Total reward: 738.0\n",
            "Episode: 94\n",
            "Total reward: 235.0 Epsilon: 0.9920475384715575 Size of replay buffer: 31968 Learn step counter: 31937\n",
            "Total reward: 235.0\n",
            "Episode: 95\n",
            "Total reward: 240.0 Epsilon: 0.9920388580924863 Size of replay buffer: 32003 Learn step counter: 31972\n",
            "Total reward: 240.0\n",
            "Episode: 96\n",
            "Total reward: 1224.0 Epsilon: 0.9918702257996591 Size of replay buffer: 32683 Learn step counter: 32652\n",
            "Total reward: 1224.0\n",
            "Episode: 97\n",
            "Total reward: 770.0 Epsilon: 0.9917797217595384 Size of replay buffer: 33048 Learn step counter: 33017\n",
            "Total reward: 770.0\n",
            "Episode: 98\n",
            "Total reward: 768.0 Epsilon: 0.9917365802745773 Size of replay buffer: 33222 Learn step counter: 33191\n",
            "Total reward: 768.0\n",
            "Episode: 99\n",
            "Total reward: 1058.0 Epsilon: 0.991694680283961 Size of replay buffer: 33391 Learn step counter: 33360\n",
            "Total reward: 1058.0\n",
            "Episode: 100\n",
            "Total reward: 856.0 Epsilon: 0.9913991992611187 Size of replay buffer: 34583 Learn step counter: 34552\n",
            "Total reward: 856.0\n",
            "Episode: 101\n",
            "Total reward: 807.0 Epsilon: 0.9913538437795915 Size of replay buffer: 34766 Learn step counter: 34735\n",
            "Total reward: 807.0\n",
            "Episode: 102\n",
            "Total reward: 632.0 Epsilon: 0.9911900360713873 Size of replay buffer: 35427 Learn step counter: 35396\n",
            "Total reward: 632.0\n",
            "Episode: 103\n",
            "Total reward: 786.0 Epsilon: 0.9908362443398183 Size of replay buffer: 36855 Learn step counter: 36824\n",
            "Total reward: 786.0\n",
            "Episode: 104\n",
            "Total reward: 791.0 Epsilon: 0.9907715943760126 Size of replay buffer: 37116 Learn step counter: 37085\n",
            "Total reward: 791.0\n",
            "Episode: 105\n",
            "Total reward: 237.0 Epsilon: 0.9907629251614049 Size of replay buffer: 37151 Learn step counter: 37120\n",
            "Total reward: 237.0\n",
            "Episode: 106\n",
            "Total reward: 946.0 Epsilon: 0.9906026821982067 Size of replay buffer: 37798 Learn step counter: 37767\n",
            "Total reward: 946.0\n",
            "Episode: 107\n",
            "Total reward: 693.0 Epsilon: 0.9904657408265461 Size of replay buffer: 38351 Learn step counter: 38320\n",
            "Total reward: 693.0\n",
            "Episode: 108\n",
            "Total reward: 640.0 Epsilon: 0.99043181795161 Size of replay buffer: 38488 Learn step counter: 38457\n",
            "Total reward: 640.0\n",
            "Episode: 109\n",
            "Total reward: 633.0 Epsilon: 0.9904117619078547 Size of replay buffer: 38569 Learn step counter: 38538\n",
            "Total reward: 633.0\n",
            "Episode: 110\n",
            "Total reward: 625.0 Epsilon: 0.990358528699646 Size of replay buffer: 38784 Learn step counter: 38753\n",
            "Total reward: 625.0\n",
            "Episode: 111\n",
            "Total reward: 951.0 Epsilon: 0.9901847360068662 Size of replay buffer: 39486 Learn step counter: 39455\n",
            "Total reward: 951.0\n",
            "Episode: 112\n",
            "Total reward: 224.0 Epsilon: 0.9901748342077761 Size of replay buffer: 39526 Learn step counter: 39495\n",
            "Total reward: 224.0\n",
            "Episode: 113\n",
            "Total reward: 246.0 Epsilon: 0.9901666652980685 Size of replay buffer: 39559 Learn step counter: 39528\n",
            "Total reward: 246.0\n",
            "Episode: 114\n",
            "Total reward: 676.0 Epsilon: 0.9900636933066281 Size of replay buffer: 39975 Learn step counter: 39944\n",
            "Total reward: 676.0\n",
            "Episode: 115\n",
            "Total reward: 242.0 Epsilon: 0.9900542877450412 Size of replay buffer: 40013 Learn step counter: 39982\n",
            "Total reward: 242.0\n",
            "Episode: 116\n",
            "Total reward: 899.0 Epsilon: 0.9898305607301175 Size of replay buffer: 40917 Learn step counter: 40886\n",
            "Total reward: 899.0\n",
            "Episode: 117\n",
            "Total reward: 1308.0 Epsilon: 0.9896370677458369 Size of replay buffer: 41699 Learn step counter: 41668\n",
            "Total reward: 1308.0\n",
            "Episode: 118\n",
            "Total reward: 223.0 Epsilon: 0.9896251921707915 Size of replay buffer: 41747 Learn step counter: 41716\n",
            "Total reward: 223.0\n",
            "Episode: 119\n",
            "Total reward: 631.0 Epsilon: 0.9896009466475582 Size of replay buffer: 41845 Learn step counter: 41814\n",
            "Total reward: 631.0\n",
            "Episode: 120\n",
            "Total reward: 249.0 Epsilon: 0.9895932772689808 Size of replay buffer: 41876 Learn step counter: 41845\n",
            "Total reward: 249.0\n",
            "Episode: 121\n",
            "Total reward: 920.0 Epsilon: 0.9894047776812245 Size of replay buffer: 42638 Learn step counter: 42607\n",
            "Total reward: 920.0\n",
            "Episode: 122\n",
            "Total reward: 235.0 Epsilon: 0.9893951310304621 Size of replay buffer: 42677 Learn step counter: 42646\n",
            "Total reward: 235.0\n",
            "Episode: 123\n",
            "Total reward: 233.0 Epsilon: 0.9893844950886408 Size of replay buffer: 42720 Learn step counter: 42689\n",
            "Total reward: 233.0\n",
            "Episode: 124\n",
            "Total reward: 805.0 Epsilon: 0.9893365110982456 Size of replay buffer: 42914 Learn step counter: 42883\n",
            "Total reward: 805.0\n",
            "Episode: 125\n",
            "Total reward: 594.0 Epsilon: 0.9893080680788627 Size of replay buffer: 43029 Learn step counter: 42998\n",
            "Total reward: 594.0\n",
            "Episode: 126\n",
            "Total reward: 636.0 Epsilon: 0.9892689911770575 Size of replay buffer: 43187 Learn step counter: 43156\n",
            "Total reward: 636.0\n",
            "Episode: 127\n",
            "Total reward: 717.0 Epsilon: 0.9891779826049388 Size of replay buffer: 43555 Learn step counter: 43524\n",
            "Total reward: 717.0\n",
            "Episode: 128\n",
            "Total reward: 886.0 Epsilon: 0.9889383832313151 Size of replay buffer: 44524 Learn step counter: 44493\n",
            "Total reward: 886.0\n",
            "Episode: 129\n",
            "Total reward: 620.0 Epsilon: 0.9889168740527015 Size of replay buffer: 44611 Learn step counter: 44580\n",
            "Total reward: 620.0\n",
            "Episode: 130\n",
            "Total reward: 1276.0 Epsilon: 0.9888221897832443 Size of replay buffer: 44994 Learn step counter: 44963\n",
            "Total reward: 1276.0\n",
            "Episode: 131\n",
            "Total reward: 1298.0 Epsilon: 0.9887487724521178 Size of replay buffer: 45291 Learn step counter: 45260\n",
            "Total reward: 1298.0\n",
            "Episode: 132\n",
            "Total reward: 236.0 Epsilon: 0.9887393793822212 Size of replay buffer: 45329 Learn step counter: 45298\n",
            "Total reward: 236.0\n",
            "Episode: 133\n",
            "Total reward: 634.0 Epsilon: 0.9887168858143907 Size of replay buffer: 45420 Learn step counter: 45389\n",
            "Total reward: 634.0\n",
            "Episode: 134\n",
            "Total reward: 769.0 Epsilon: 0.9886241979395585 Size of replay buffer: 45795 Learn step counter: 45764\n",
            "Total reward: 769.0\n",
            "Episode: 135\n",
            "Total reward: 602.0 Epsilon: 0.9886014598416496 Size of replay buffer: 45887 Learn step counter: 45856\n",
            "Total reward: 602.0\n",
            "Episode: 136\n",
            "Total reward: 729.0 Epsilon: 0.9884603470377651 Size of replay buffer: 46458 Learn step counter: 46427\n",
            "Total reward: 729.0\n",
            "Episode: 137\n",
            "Total reward: 608.0 Epsilon: 0.9884319292077396 Size of replay buffer: 46573 Learn step counter: 46542\n",
            "Total reward: 608.0\n",
            "Episode: 138\n",
            "Total reward: 637.0 Epsilon: 0.9883938753062456 Size of replay buffer: 46727 Learn step counter: 46696\n",
            "Total reward: 637.0\n",
            "Episode: 139\n",
            "Total reward: 597.0 Epsilon: 0.9883654593872555 Size of replay buffer: 46842 Learn step counter: 46811\n",
            "Total reward: 597.0\n",
            "Episode: 140\n",
            "Total reward: 637.0 Epsilon: 0.9883276551267144 Size of replay buffer: 46995 Learn step counter: 46964\n",
            "Total reward: 637.0\n",
            "Episode: 141\n",
            "Total reward: 978.0 Epsilon: 0.9882028866208193 Size of replay buffer: 47500 Learn step counter: 47469\n",
            "Total reward: 978.0\n",
            "Episode: 142\n",
            "Total reward: 645.0 Epsilon: 0.9879578426609791 Size of replay buffer: 48492 Learn step counter: 48461\n",
            "Total reward: 645.0\n",
            "Episode: 143\n",
            "Total reward: 229.0 Epsilon: 0.9879472221699269 Size of replay buffer: 48535 Learn step counter: 48504\n",
            "Total reward: 229.0\n",
            "Episode: 144\n",
            "Total reward: 586.0 Epsilon: 0.9879200539914816 Size of replay buffer: 48645 Learn step counter: 48614\n",
            "Total reward: 586.0\n",
            "Episode: 145\n",
            "Total reward: 580.0 Epsilon: 0.9878884410516074 Size of replay buffer: 48773 Learn step counter: 48742\n",
            "Total reward: 580.0\n",
            "Episode: 146\n",
            "Total reward: 713.0 Epsilon: 0.9878375661005677 Size of replay buffer: 48979 Learn step counter: 48948\n",
            "Total reward: 713.0\n",
            "Episode: 147\n",
            "Total reward: 590.0 Epsilon: 0.987812623513804 Size of replay buffer: 49080 Learn step counter: 49049\n",
            "Total reward: 590.0\n",
            "Episode: 148\n",
            "Total reward: 240.0 Epsilon: 0.9878032393372811 Size of replay buffer: 49118 Learn step counter: 49087\n",
            "Total reward: 240.0\n",
            "Episode: 149\n",
            "Total reward: 240.0 Epsilon: 0.9877938552499075 Size of replay buffer: 49156 Learn step counter: 49125\n",
            "Total reward: 240.0\n",
            "Episode: 150\n",
            "Total reward: 233.0 Epsilon: 0.9877839773595083 Size of replay buffer: 49196 Learn step counter: 49165\n",
            "Total reward: 233.0\n",
            "Episode: 151\n",
            "Total reward: 627.0 Epsilon: 0.9877607647058855 Size of replay buffer: 49290 Learn step counter: 49259\n",
            "Total reward: 627.0\n",
            "Episode: 152\n",
            "Total reward: 1239.0 Epsilon: 0.987621006428324 Size of replay buffer: 49856 Learn step counter: 49825\n",
            "Total reward: 1239.0\n",
            "Episode: 153\n",
            "Total reward: 238.0 Epsilon: 0.9876116240721549 Size of replay buffer: 49894 Learn step counter: 49863\n",
            "Total reward: 238.0\n",
            "Episode: 154\n",
            "Total reward: 1309.0 Epsilon: 0.9875266931139397 Size of replay buffer: 50238 Learn step counter: 50207\n",
            "Total reward: 1309.0\n",
            "Episode: 155\n",
            "Total reward: 1303.0 Epsilon: 0.9871515042140117 Size of replay buffer: 51758 Learn step counter: 51727\n",
            "Total reward: 1303.0\n",
            "Episode: 156\n",
            "Total reward: 584.0 Epsilon: 0.9871236175744285 Size of replay buffer: 51871 Learn step counter: 51840\n",
            "Total reward: 584.0\n",
            "Episode: 157\n",
            "Total reward: 227.0 Epsilon: 0.9871134996079368 Size of replay buffer: 51912 Learn step counter: 51881\n",
            "Total reward: 227.0\n",
            "Episode: 158\n",
            "Total reward: 1255.0 Epsilon: 0.9869859234169364 Size of replay buffer: 52429 Learn step counter: 52398\n",
            "Total reward: 1255.0\n",
            "Episode: 159\n",
            "Total reward: 605.0 Epsilon: 0.9869664306350012 Size of replay buffer: 52508 Learn step counter: 52477\n",
            "Total reward: 605.0\n",
            "Episode: 160\n",
            "Total reward: 520.0 Epsilon: 0.9865680233226635 Size of replay buffer: 54123 Learn step counter: 54092\n",
            "Total reward: 520.0\n",
            "Episode: 161\n",
            "Total reward: 712.0 Epsilon: 0.9864054996087415 Size of replay buffer: 54782 Learn step counter: 54751\n",
            "Total reward: 712.0\n",
            "Episode: 162\n",
            "Total reward: 249.0 Epsilon: 0.9863973617959199 Size of replay buffer: 54815 Learn step counter: 54784\n",
            "Total reward: 249.0\n",
            "Episode: 163\n",
            "Total reward: 599.0 Epsilon: 0.9863657975814224 Size of replay buffer: 54943 Learn step counter: 54912\n",
            "Total reward: 599.0\n",
            "Episode: 164\n",
            "Total reward: 628.0 Epsilon: 0.9863403989859645 Size of replay buffer: 55046 Learn step counter: 55015\n",
            "Total reward: 628.0\n",
            "Episode: 165\n",
            "Total reward: 242.0 Epsilon: 0.9863312753783289 Size of replay buffer: 55083 Learn step counter: 55052\n",
            "Total reward: 242.0\n",
            "Episode: 166\n",
            "Total reward: 249.0 Epsilon: 0.9863228915970702 Size of replay buffer: 55117 Learn step counter: 55086\n",
            "Total reward: 249.0\n",
            "Episode: 167\n",
            "Total reward: 752.0 Epsilon: 0.9862107037349177 Size of replay buffer: 55572 Learn step counter: 55541\n",
            "Total reward: 752.0\n",
            "Episode: 168\n",
            "Total reward: 1038.0 Epsilon: 0.98617347497889 Size of replay buffer: 55723 Learn step counter: 55692\n",
            "Total reward: 1038.0\n",
            "Episode: 169\n",
            "Total reward: 802.0 Epsilon: 0.9861224418156678 Size of replay buffer: 55930 Learn step counter: 55899\n",
            "Total reward: 802.0\n",
            "Episode: 170\n",
            "Total reward: 1097.0 Epsilon: 0.9857983073048875 Size of replay buffer: 57245 Learn step counter: 57214\n",
            "Total reward: 1097.0\n",
            "Episode: 171\n",
            "Total reward: 234.0 Epsilon: 0.9857896816063568 Size of replay buffer: 57280 Learn step counter: 57249\n",
            "Total reward: 234.0\n",
            "Episode: 172\n",
            "Total reward: 618.0 Epsilon: 0.9857702124499671 Size of replay buffer: 57359 Learn step counter: 57328\n",
            "Total reward: 618.0\n",
            "Episode: 173\n",
            "Total reward: 611.0 Epsilon: 0.9857472935560928 Size of replay buffer: 57452 Learn step counter: 57421\n",
            "Total reward: 611.0\n",
            "Episode: 174\n",
            "Total reward: 247.0 Epsilon: 0.9857391611734494 Size of replay buffer: 57485 Learn step counter: 57454\n",
            "Total reward: 247.0\n",
            "Episode: 175\n",
            "Total reward: 244.0 Epsilon: 0.9857305359924451 Size of replay buffer: 57520 Learn step counter: 57489\n",
            "Total reward: 244.0\n",
            "Episode: 176\n",
            "Total reward: 784.0 Epsilon: 0.985657841036978 Size of replay buffer: 57815 Learn step counter: 57784\n",
            "Total reward: 784.0\n",
            "Episode: 177\n",
            "Total reward: 674.0 Epsilon: 0.9854486573344464 Size of replay buffer: 58664 Learn step counter: 58633\n",
            "Total reward: 674.0\n",
            "Episode: 178\n",
            "Total reward: 615.0 Epsilon: 0.9854102255814248 Size of replay buffer: 58820 Learn step counter: 58789\n",
            "Total reward: 615.0\n",
            "Episode: 179\n",
            "Total reward: 236.0 Epsilon: 0.985400371527206 Size of replay buffer: 58860 Learn step counter: 58829\n",
            "Total reward: 236.0\n",
            "Episode: 180\n",
            "Total reward: 742.0 Epsilon: 0.9852752336107888 Size of replay buffer: 59368 Learn step counter: 59337\n",
            "Total reward: 742.0\n",
            "Episode: 181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g1SofYkIcRcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}